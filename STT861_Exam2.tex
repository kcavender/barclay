\documentclass[14pt, oneside, letterpaper]{notes}
\usepackage{mathtools}

\begin{document}
\title{STT 861 Compendium - Part Two}
\author{Kenyon Cavender}
\maketitle

\section*{Expected Values}

\begin{mydef}
	The \textbf{expected value} of a random variable $g(X)$: \\
	\[
	\mathbb{E}(g(X)) = 
		\begin{dcases}
			\int_{-\infty}^{\infty} g(x)f_X(x)dx 
			& \text{if $X$ is continuous} \\			
			\sum_{X}g(x)f_X(x)
			& \text{if $X$ is discrete}
		\end{dcases}
	\]
\end{mydef}

\begin{remark}
If $-\infty \leq \mathbb{E}(g(x)) \leq \infty$ we say the
expectation of $g(x)$ exists.  Else it does not exist. \\
\indent notation: $\left| \mathbb{E}(g(x)) \right| 
\leq \infty$ \\
\indent In particular, if $g(x) = x$, then we get the expected
value of $X$: \\
	\[
	\mathbb{E}(g(X)) = 
		\begin{dcases}
			\int xf_X(x)dx 
			& \text{if $X$ is continuous} \\			
			\sum xf_X(x)
			& \text{if $X$ is discrete}
		\end{dcases}
	\]
\indent This is called the \textbf{mean} of r.v. $X$ \\
\indent Also denoted by $\mu$ or $\mu_X$
\end{remark}

\begin{theorem} Expectation 
\begin{enumerate}
	\item $\mathbb{E}(ag(x) + b) = a\mathbb{E}(g(x)) + b
		\text{$a$,$b$ real constants}$
	\item $\text{If} g(x) \geq 0 \text{for all} x \in 
		\mathbb{R} \text{, then} \mathbb{E}(g(x)) \geq 0$
	\item $\text{If} g_1(x) \geq g_2(x) \text{for all} 
		x \in \mathbb{R} \text{, then} 
		\mathbb{E}(g_1(x)) \geq \mathbb{E}(g_2(x))$
	\item For any real constants $a$,$b$ if $a \leq X 
		\leq b$ then $a < \mathbb{E}(X) < b$
\end{enumerate}
\end{theorem}

\begin{remark} Expectation
	\begin{enumerate}
	\item If $g$ is a linear fn, $\mathbb{E}[g(x)] =
	g[\mathbb{E}(x)]$
	\item $g(x)$ has finite expectation if
	$0 \leq \mathbb{E}[\left| g(x) \right|] \le \infty $
	\end{enumerate}
\end{remark}


\section*{Moments}
\begin{mydef}
	\textbf{Moments} For a r.v. $X$, we define the $r^{th}$
	raw moments by 
	\[ 
		\mu'_r = 
		\begin{dcases}
			\int x'f_X(x)dx 
			& \text{if $X$ is continuous} \\			
			\sum x'f_X(x)
			& \text{if $X$ is discrete}
		\end{dcases}
	\]
	$\mu'_1 = \mathbb{E}(x') = \mathbb{E}(x) = \mu$
\end{mydef}

\begin{mydef}
	The $r^{th}$ central moment is defined as 
	$\mu_r = \mathbb{E}[(x-\mu)^r] $
\end{mydef}

\begin{theorem} 2.3.11 \\
Let $F_X(x)$ and $F_Y(y)$ be two cdfs all of whose moments
exist.  If $X$, $Y$ have bounded support \\
(i.e. Support$(f_X) = \{x: f_X(x)=0 \}$) \\
Then $F_X(u) = F_Y(u) \: \forall \: u$ iff $\mathbb{E}(x')
= \mathbb{E}(y') \: \forall \: r \in \mathbb{Z}$ \\
Example: P(Heads) for a coin flip and P(Even) for a die
roll have the same distribution, but are different variables.
\end{theorem}

\begin{mydef}
\textbf{Moment Generating Function}: For a r.v. $X$,
the mgf is defined as: $M_X(t) = \mathbb{E}(e^{tx})$ 
provided the expectation exists for all t in a neighborhood
of 0.  I.e. $\mathbb{E} < \infty \: \forall \: t \in
(-h,h) \text{for some} h>0$
\end{mydef}


\end{document}























